{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import shortuuid\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from fastchat.llm_judge.common import load_questions\n",
    "from fastchat.model import load_model, get_conversation_template\n",
    "\n",
    "import transformers\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_forward(input_ids, model, tokenizer, max_new_token, temperature, top_p, max_steps=1024):\n",
    "    assert input_ids.shape[0] == 1, \"Only support batch size 1 for now!!\"\n",
    "    # Avoid modifying the input_ids in-place\n",
    "    input_ids = input_ids.clone()\n",
    "    accept_length_list = []\n",
    "\n",
    "    # Initialize the past key and value states\n",
    "    if hasattr(model, \"past_key_values\"):\n",
    "        past_key_values = model.past_key_values\n",
    "        past_key_values_data = model.past_key_values_data\n",
    "        current_length_data = model.current_length_data\n",
    "        # Reset the past key and value states\n",
    "        current_length_data.zero_()\n",
    "    else:\n",
    "        (\n",
    "            past_key_values,\n",
    "            past_key_values_data,\n",
    "            current_length_data,\n",
    "        ) = initialize_past_key_values(model.base_model)\n",
    "        model.past_key_values = past_key_values\n",
    "        model.past_key_values_data = past_key_values_data\n",
    "        model.current_length_data = current_length_data\n",
    "\n",
    "    input_len = input_ids.shape[1]\n",
    "    model.base_model.model.draft_mask = None\n",
    "    outputs = model.base_model(input_ids, past_key_values = past_key_values, use_cache=True)\n",
    "    new_token = 0\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    start_time = time.time()\n",
    "    for idx in range(max_steps): \n",
    "        # candidates, tree_candidates, draft_buffers = generate_candidates_and_draft_buffer(\n",
    "        #         logits,\n",
    "        #         input_ids,\n",
    "        #         datastore,\n",
    "        #         token_spans,\n",
    "        #         top_p,\n",
    "        #         temperature,\n",
    "        #         max_num_draft=num_draft,\n",
    "        #         device=model.base_model.device\n",
    "        #     )\n",
    "        # model.base_model.model.draft_mask = draft_buffers[\"draft_attn_mask\"]\n",
    "        # logits, outputs = tree_decoding(\n",
    "        #         model,\n",
    "        #         tree_candidates,\n",
    "        #         past_key_values,\n",
    "        #         draft_buffers[\"draft_position_ids\"],\n",
    "        #         input_ids,\n",
    "        #         draft_buffers[\"retrieve_indices\"],\n",
    "        #     )\n",
    "        # best_candidate, accept_length = evaluate_posterior(\n",
    "        #         logits, candidates, temperature, top_p\n",
    "        #     )\n",
    "        # input_ids, logits, new_token = update_inference_inputs(\n",
    "        #         input_ids,\n",
    "        #         candidates,\n",
    "        #         best_candidate,\n",
    "        #         accept_length,\n",
    "        #         draft_buffers[\"retrieve_indices\"],\n",
    "        #         outputs,\n",
    "        #         logits,\n",
    "        #         new_token,\n",
    "        #         past_key_values_data,\n",
    "        #         current_length_data,\n",
    "        #     )\n",
    "        # accept_length_tree = input_ids.shape[1] - cur_length\n",
    "        if top_p > 0:\n",
    "            assert top_p < 1, \"top_p should between 0.0 and 1\"\n",
    "            next_token_logits = outputs.logits[:, -1, :]\n",
    "            next_token_logits = next_token_logits / (temperature if temperature > 0 else 1.)\n",
    "            filtered_logits = top_p_filtering(next_token_logits, top_p=top_p)\n",
    "            input_id = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples=1)\n",
    "            input_id = input_id.view(input_id.shape[0], 1)\n",
    "        else:\n",
    "            input_id = outputs.logits[:, -1:].argmax(dim=-1)\n",
    "        outputs = model.base_model(input_id, use_cache=True, past_key_values = past_key_values)\n",
    "        input_ids = torch.cat([input_ids, input_id], dim=-1)\n",
    "        new_token += 1\n",
    "        accept_length_list.append(1)\n",
    "        if tokenizer.eos_token_id in input_ids[0, input_len:].tolist():\n",
    "            break\n",
    "        if new_token > max_new_token:\n",
    "            break\n",
    "    return input_ids, new_token, idx, accept_length_list, start_time\n",
    "\n",
    "def run_eval(\n",
    "    model_path,\n",
    "    model_id,\n",
    "    question_file,\n",
    "    question_begin,\n",
    "    question_end,\n",
    "    answer_file,\n",
    "    max_new_token,\n",
    "    num_choices,\n",
    "    num_gpus_per_model,\n",
    "    num_gpus_total,\n",
    "    max_gpu_memory,\n",
    "    temperature,\n",
    "    top_p,\n",
    "):\n",
    "    questions = load_questions(question_file, question_begin, question_end)\n",
    "    # random shuffle the questions to balance the loading\n",
    "    # random.shuffle(questions)\n",
    "    shuffled_ids = [q[\"question_id\"] for q in questions]\n",
    "    # with open(f\"data/{args.bench_name}/model_ids/{args.model_id}.shuffled_ids\", \"w\") as fout:\n",
    "    #     json.dump(shuffled_ids, fout)\n",
    "\n",
    "    # Split the question file into `num_gpus` files\n",
    "    assert num_gpus_total % num_gpus_per_model == 0\n",
    "    use_ray = num_gpus_total // num_gpus_per_model > 1\n",
    "\n",
    "    if use_ray:\n",
    "        get_answers_func = ray.remote(num_gpus=num_gpus_per_model)(\n",
    "            get_model_answers\n",
    "        ).remote\n",
    "    else:\n",
    "        get_answers_func = get_model_answers\n",
    "\n",
    "    chunk_size = len(questions) // (num_gpus_total // num_gpus_per_model) # // 2\n",
    "    ans_handles = []\n",
    "    for i in range(0, len(questions), chunk_size):\n",
    "        ans_handles.append(\n",
    "            get_answers_func(\n",
    "                model_path,\n",
    "                model_id,\n",
    "                questions[i : i + chunk_size],\n",
    "                answer_file,\n",
    "                max_new_token,\n",
    "                num_choices,\n",
    "                num_gpus_per_model,\n",
    "                max_gpu_memory,\n",
    "                temperature,\n",
    "                top_p,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if use_ray:\n",
    "        ray.get(ans_handles)\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def get_model_answers(\n",
    "    model_path,\n",
    "    model_id,\n",
    "    questions,\n",
    "    answer_file,\n",
    "    max_new_token,\n",
    "    num_choices,\n",
    "    num_gpus_per_model,\n",
    "    max_gpu_memory,\n",
    "    temperature,\n",
    "    top_p,\n",
    "):\n",
    "    \n",
    "    model = RestModel.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype=torch.float16,\n",
    "        low_cpu_mem_usage=True,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    tokenizer = model.get_tokenizer()\n",
    "    \n",
    "    model.eval()\n",
    "    print('Check model training state:',model.training)\n",
    "    \n",
    "    cuda_visible_devices = os.environ.get('CUDA_VISIBLE_DEVICES')\n",
    "    print('CUDA VISIBLE DEVICES:', cuda_visible_devices)\n",
    "    \n",
    "    question = questions[0]\n",
    "\n",
    "    # warmup\n",
    "    for _ in range(3):\n",
    "        torch.manual_seed(0)\n",
    "        conv = get_conversation_template(model_id)\n",
    "        turns = []\n",
    "        idxs = []\n",
    "        new_tokens = []\n",
    "        wall_time = []\n",
    "        for j in range(len(question[\"turns\"])):\n",
    "            qs = question[\"turns\"][j]\n",
    "            conv.append_message(conv.roles[0], qs)\n",
    "            conv.append_message(conv.roles[1], None)\n",
    "            prompt = conv.get_prompt()\n",
    "            input_ids = tokenizer([prompt]).input_ids\n",
    "\n",
    "            # if temperature < 1e-4:\n",
    "            #     do_sample = False\n",
    "            # else:\n",
    "            #     do_sample = True\n",
    "\n",
    "            # some models may error out when generating long outputs\n",
    "            try:\n",
    "                output_ids, new_token, idx, _, start_time = baseline_forward(\n",
    "                    torch.as_tensor(input_ids).cuda(),\n",
    "                    model,\n",
    "                    tokenizer,\n",
    "                    max_new_token,\n",
    "                    temperature,\n",
    "                    top_p,\n",
    "                )\n",
    "                torch.cuda.synchronize()\n",
    "                total_time = time.time() - start_time\n",
    "                output_ids = output_ids[0][len(input_ids[0]) :]\n",
    "                # be consistent with the template's stop_token_ids\n",
    "                if conv.stop_token_ids:\n",
    "                    stop_token_ids_index = [\n",
    "                        i\n",
    "                        for i, id in enumerate(output_ids)\n",
    "                        if id in conv.stop_token_ids\n",
    "                    ]\n",
    "                    if len(stop_token_ids_index) > 0:\n",
    "                        output_ids = output_ids[: stop_token_ids_index[0]]\n",
    "\n",
    "                output = tokenizer.decode(\n",
    "                    output_ids,\n",
    "                    spaces_between_special_tokens=False,\n",
    "                )\n",
    "                if conv.stop_str and output.find(conv.stop_str) > 0:\n",
    "                    output = output[: output.find(conv.stop_str)]\n",
    "\n",
    "                if conv.name == \"xgen\" and output.startswith(\"Assistant:\"):\n",
    "                    output = output.replace(\"Assistant:\", \"\", 1).strip()\n",
    "            except RuntimeError as e:\n",
    "                print(\"ERROR question ID: \", question[\"question_id\"])\n",
    "                output = \"ERROR\"\n",
    "\n",
    "            turns.append(output)\n",
    "            idxs.append(int(idx))\n",
    "            new_tokens.append(int(new_token))\n",
    "            wall_time.append(total_time)\n",
    "            conv.messages[-1][-1] = output\n",
    "    print('Warmup done')\n",
    "\n",
    "    accept_lengths_tree = []\n",
    "    for question in tqdm(questions):\n",
    "        # if question[\"category\"] in temperature_config:\n",
    "        #     temperature = temperature_config[question[\"category\"]]\n",
    "        # else:\n",
    "        #     temperature = 0.7\n",
    "        choices = []\n",
    "        for i in range(num_choices):\n",
    "            accept_lengths_tree_this = []\n",
    "            torch.manual_seed(i)\n",
    "            conv = get_conversation_template(model_id)\n",
    "            turns = []\n",
    "            idxs = []\n",
    "            new_tokens = []\n",
    "            wall_time = []\n",
    "            for j in range(len(question[\"turns\"])):\n",
    "                qs = question[\"turns\"][j]\n",
    "                conv.append_message(conv.roles[0], qs)\n",
    "                conv.append_message(conv.roles[1], None)\n",
    "                prompt = conv.get_prompt()\n",
    "                input_ids = tokenizer([prompt]).input_ids\n",
    "\n",
    "                # if temperature < 1e-4:\n",
    "                #     do_sample = False\n",
    "                # else:\n",
    "                #     do_sample = True\n",
    "\n",
    "                # some models may error out when generating long outputs\n",
    "                try:\n",
    "\n",
    "                    output_ids, new_token, idx, accept_length_tree, start_time = baseline_forward(\n",
    "                        torch.as_tensor(input_ids).cuda(),\n",
    "                        model,\n",
    "                        tokenizer,\n",
    "                        max_new_token,\n",
    "                        temperature,\n",
    "                        top_p,\n",
    "                    )\n",
    "                    torch.cuda.synchronize()\n",
    "                    total_time = time.time() - start_time\n",
    "                    accept_lengths_tree.extend(accept_length_tree)\n",
    "                    # if model.config.is_encoder_decoder:\n",
    "                    #     output_ids = output_ids[0]\n",
    "                    # else:\n",
    "                    output_ids = output_ids[0][len(input_ids[0]) :]\n",
    "\n",
    "                    # be consistent with the template's stop_token_ids\n",
    "                    if conv.stop_token_ids:\n",
    "                        stop_token_ids_index = [\n",
    "                            i\n",
    "                            for i, id in enumerate(output_ids)\n",
    "                            if id in conv.stop_token_ids\n",
    "                        ]\n",
    "                        if len(stop_token_ids_index) > 0:\n",
    "                            output_ids = output_ids[: stop_token_ids_index[0]]\n",
    "\n",
    "                    output = tokenizer.decode(\n",
    "                        output_ids,\n",
    "                        spaces_between_special_tokens=False,\n",
    "                    )\n",
    "                    if conv.stop_str and output.find(conv.stop_str) > 0:\n",
    "                        output = output[: output.find(conv.stop_str)]\n",
    "                    # for special_token in tokenizer.special_tokens_map.values():\n",
    "                    #     if isinstance(special_token, list):\n",
    "                    #         for special_tok in special_token:\n",
    "                    #             output = output.replace(special_tok, \"\")\n",
    "                    #     else:\n",
    "                    #         output = output.replace(special_token, \"\")\n",
    "                    # output = output.strip()\n",
    "\n",
    "                    if conv.name == \"xgen\" and output.startswith(\"Assistant:\"):\n",
    "                        output = output.replace(\"Assistant:\", \"\", 1).strip()\n",
    "                except RuntimeError as e:\n",
    "                    print(\"ERROR question ID: \", question[\"question_id\"])\n",
    "                    output = \"ERROR\"\n",
    "\n",
    "                turns.append(output)\n",
    "                idxs.append(int(idx))\n",
    "                new_tokens.append(int(new_token))\n",
    "                wall_time.append(total_time)\n",
    "                accept_lengths_tree_this.extend(accept_length_tree)\n",
    "                conv.messages[-1][-1] = output\n",
    "            # torch.cuda.empty_cache()\n",
    "            choices.append({\"index\": i, \"turns\": turns, \"idxs\": idxs, \"new_tokens\": new_tokens, \"wall_time\": wall_time, \"accept_lengths:\": accept_lengths_tree_this})\n",
    "\n",
    "        # Dump answers\n",
    "        os.makedirs(os.path.dirname(answer_file), exist_ok=True)\n",
    "        with open(os.path.expanduser(answer_file), \"a\") as fout:\n",
    "            ans_json = {\n",
    "                \"category\": question[\"category\"],\n",
    "                \"question_id\": question[\"question_id\"],\n",
    "                \"answer_id\": shortuuid.uuid(),\n",
    "                \"model_id\": model_id,\n",
    "                \"choices\": choices,\n",
    "                \"tstamp\": time.time(),\n",
    "            }\n",
    "            fout.write(json.dumps(ans_json) + \"\\n\")\n",
    "    print(\"accept_lengths_tree: \", np.mean(accept_lengths_tree))\n",
    "\n",
    "\n",
    "def reorg_answer_file(answer_file):\n",
    "    \"\"\"Sort by question id and de-duplication\"\"\"\n",
    "    answers = {}\n",
    "    with open(answer_file, \"r\") as fin:\n",
    "        for l in fin:\n",
    "            qid = json.loads(l)[\"question_id\"]\n",
    "            answers[qid] = l\n",
    "\n",
    "    qids = sorted(list(answers.keys()))\n",
    "    with open(answer_file, \"w\") as fout:\n",
    "        for qid in qids:\n",
    "            fout.write(answers[qid])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--model-path\",\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help=\"The path to the weights. This can be a local folder or a Hugging Face repo ID.\",\n",
    "    )\n",
    "    parser.add_argument(\"--model-id\", type=str, required=True)\n",
    "    parser.add_argument(\n",
    "        \"--bench-name\",\n",
    "        type=str,\n",
    "        default=\"mt_bench\",\n",
    "        help=\"The name of the benchmark question set.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--question-begin\",\n",
    "        type=int,\n",
    "        help=\"A debug option. The begin index of questions.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--question-end\", type=int, help=\"A debug option. The end index of questions.\"\n",
    "    )\n",
    "    parser.add_argument(\"--answer-file\", type=str, help=\"The output answer file.\")\n",
    "    parser.add_argument(\n",
    "        \"--max-new-token\",\n",
    "        type=int,\n",
    "        default=1024,\n",
    "        help=\"The maximum number of new generated tokens.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num-choices\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help=\"How many completion choices to generate.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num-gpus-per-model\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help=\"The number of GPUs per model.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num-gpus-total\", type=int, default=1, help=\"The total number of GPUs.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max-gpu-memory\",\n",
    "        type=str,\n",
    "        help=\"Maxmum GPU memory used for model weights per GPU.\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--temperature\",\n",
    "        type=float,\n",
    "        default=0.0,\n",
    "        help=\"The temperature for sampling.\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--top-p\",\n",
    "        type=float,\n",
    "        default=0.0,\n",
    "        help=\"The threshold for nucleus sampling.\",\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.temperature == 0:\n",
    "        args.top_p = 0\n",
    "        \n",
    "\n",
    "    args.model_id = \"baseline-\" + args.model_id+\"-temperature-\"+str(args.temperature)+\"-top_p-\"+str(args.top_p)\n",
    "    if args.num_gpus_total // args.num_gpus_per_model > 1:\n",
    "        import ray\n",
    "        ray.init()\n",
    "\n",
    "    question_file = f\"data/{args.bench_name}/question.jsonl\"\n",
    "    if args.answer_file:\n",
    "        answer_file = args.answer_file\n",
    "    else:\n",
    "        answer_file = f\"data/{args.bench_name}/model_answer/{args.model_id}.jsonl\"\n",
    "\n",
    "    print(f\"Output to {answer_file}\")\n",
    "\n",
    "    run_eval(\n",
    "        args.model_path,\n",
    "        args.model_id,\n",
    "        question_file,\n",
    "        args.question_begin,\n",
    "        args.question_end,\n",
    "        answer_file,\n",
    "        args.max_new_token,\n",
    "        args.num_choices,\n",
    "        args.num_gpus_per_model,\n",
    "        args.num_gpus_total,\n",
    "        args.max_gpu_memory,\n",
    "        args.temperature,\n",
    "        args.top_p,\n",
    "    )\n",
    "\n",
    "    reorg_answer_file(answer_file)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
